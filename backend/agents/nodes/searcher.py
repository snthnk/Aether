import os
import urllib
import time
import requests
from bs4 import BeautifulSoup
import re
import xml.etree.ElementTree as ET
from typing import List, Optional, Dict
from backend.llm.llms import llm, llm_creative
from backend.agents.prompts import (
    SEARCH_QUERY_PLANNER_PROMPT,
    SEARCH_QUERY_PLANNER_CREATIVE_PROMPT,
    SEARCH_SUMMARIZER_PROMPT,
    VALIDATION_PROMPT
)
import pymupdf as fitz
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from backend.agents.constants import MIN_VALIDATED_ARTICLES, MAX_SEARCH_CYCLES, MAX_ARTICLES_COUNT
from backend.agents.classes import SearchRequest, GraphState
from backend.token_count import token_count


def download_arxiv_html_article(article_id: str) -> Optional[str]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç HTML-–≤–µ—Ä—Å–∏—é —Å—Ç–∞—Ç—å–∏ —Å arXiv –≤ Markdown."""
    print(f"    [*] –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å HTML-–≤–µ—Ä—Å–∏—é —Å—Ç–∞—Ç—å–∏ {article_id}...")
    url = f"https://arxiv.org/html/{article_id}"
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        article_body = soup.find('div', class_='ltx_document')
        if not article_body:
            print("    [!] –ù–µ –Ω–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ –≤ HTML.")
            return None
        for math in article_body.find_all('math'):
            if 'alttext' in math.attrs: math.replace_with(f"`{math['alttext'].strip()}`")
        for unwanted in article_body.find_all(['figure', 'table', 'aside', 'button', 'nav']): unwanted.decompose()
        for h_level in range(1, 4):
            for header in article_body.find_all(f'h{h_level}'): header.replace_with(
                f"\n{'#' * h_level} {header.get_text().strip()}\n")
        abstract = article_body.find('div', class_='ltx_abstract')
        if abstract: abstract.replace_with(f"\n## Abstract\n{abstract.get_text().strip()}\n")
        for p in article_body.find_all('p'): p.replace_with(f"{p.get_text().strip()}\n\n")
        clean_text = article_body.get_text()
        clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text).strip()
        print("    [+] HTML-–≤–µ—Ä—Å–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ Markdown.")
        return clean_text
    except requests.RequestException as e:
        print(f"    [!] –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ HTML: {e}")
        return None
    except Exception as e:
        print(f"    [!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML: {e}")
        return None


# ========================= –ü–õ–ê–ù–ò–†–û–í–©–ò–ö –ó–ê–ü–†–û–°–û–í (–¶–ò–ö–õ–ò–ß–ï–°–ö–ò–ô) =========================
class SearchQueryPlanner(BaseModel):
    queries: List[str] = Field(description="A list of 3-4 very short and concise search queries in English.")


def plan_search_queries_node(state: GraphState) -> GraphState:
    global token_count
    cycle_count = state['search_cycles'] + 1
    print(f"\n--- üß† –ê–ì–ï–ù–¢-–ü–õ–ê–ù–ò–†–û–í–©–ò–ö (–¶–ò–ö–õ {cycle_count}/{MAX_SEARCH_CYCLES}) ---")
    parser = JsonOutputParser(pydantic_object=SearchQueryPlanner)

    state['papers'] = []
    previous_queries = []
    for search_req in state['search_history']:
        previous_queries.extend(search_req.search_queries)

    if not previous_queries:
        prompt = ChatPromptTemplate.from_template(SEARCH_QUERY_PLANNER_PROMPT)
        llm_chain = prompt | llm
    else:
        print(f"  [i] –ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –¥–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {previous_queries}")
        print("  [*] –ì–µ–Ω–µ—Ä–∏—Ä—É—é –Ω–æ–≤—ã–µ, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã...")
        prompt = ChatPromptTemplate.from_template(SEARCH_QUERY_PLANNER_CREATIVE_PROMPT)
        llm_chain = prompt | llm_creative


    chain_input = {
        "query": state['current_search_request'].input_query,
        "previous_queries_str": "\n- ".join(previous_queries),
        "format_instructions": parser.get_format_instructions()
    }

    try:
        llm_response = llm_chain.invoke(chain_input)
        if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
            token_count += llm_response.usage_metadata.get('total_tokens', 0)
        plan = parser.parse(llm_response.content)
    except Exception as e:
        print(f"  [!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞ –ø–æ–∏—Å–∫–∞: {e}")
        time.sleep(20)
        llm_response = llm_chain.invoke(chain_input)
        if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
            token_count += llm_response.usage_metadata.get('total_tokens', 0)
        plan = parser.parse(llm_response.content)

    new_queries = plan['queries']
    print(f"  [+] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–æ–≤—ã–π –ø–ª–∞–Ω –ø–æ–∏—Å–∫–∞:")
    for q in new_queries: print(f"    - {q}")

    state['current_search_request'].search_queries = new_queries
    state['search_cycles'] = cycle_count

    return state




# ========================= –£–ó–õ–´ –ü–û–ò–°–ö–ê =========================
def search_openalex_node(state: GraphState) -> GraphState:
    print("\n--- üîç –ò–©–£ –°–¢–ê–¢–¨–ò –í OPENALEX ---")
    search_queries = state['current_search_request'].search_queries if state['current_search_request'] else []
    all_results = state['papers']
    seen_ids = {p.get("id") for p in all_results if p.get("id")}

    for query in search_queries:
        print(f"  [*] –ó–∞–ø—Ä–æ—Å –≤ OpenAlex: '{query}'")
        your_email = "senya.novozhilov@gmail.com"
        url = "https://api.openalex.org/works"
        params = {
            'search': query,
            'mailto': your_email,
            'per_page': 10  # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–º–∞–∫—Å. 200)
        }
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            results = response.json().get("results", [])
            new_papers_count = 0
            for paper in results:
                if paper.get("id") not in seen_ids:
                    all_results.append(paper)
                    seen_ids.add(paper.get("id"))
                    new_papers_count += 1
            print(f"    [+] –ù–∞–π–¥–µ–Ω–æ {new_papers_count} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π.")
        except Exception as e:
            print(f"    [!] –û—à–∏–±–∫–∞: {e}")

    state['papers'] = all_results
    return state


def search_arxiv_node(state: GraphState) -> GraphState:
    print("\n--- üìö –ò–©–£ –°–¢–ê–¢–¨–ò –í ARXIV ---")
    search_queries = state['current_search_request'].search_queries if state['current_search_request'] else []
    all_results = state['papers']
    seen_titles = set()
    if all_results:
        for p in all_results:
            if p is None or p.get("title", "") is None:
                continue
            seen_titles.add(p.get("title", "").lower().strip())

    for query in search_queries:
        print(f"  [*] –ó–∞–ø—Ä–æ—Å –≤ arXiv: '{query}'")
        try:
            url = f"http://export.arxiv.org/api/query?search_query=all:{urllib.parse.quote_plus(f'\"{query}\"')}&start=0&max_results=3&sortBy=relevance"
            response = requests.get(url, timeout=20)
            response.raise_for_status()
            root = ET.fromstring(response.content)
            atom_ns = '{http://www.w3.org/2005/Atom}'
            new_papers_count = 0
            for entry in root.findall(f'{atom_ns}entry'):
                title = entry.find(f'{atom_ns}title').text.strip().replace('\n', ' ')
                if title.lower() in seen_titles: continue
                authors = [a.find(f'{atom_ns}name').text for a in entry.findall(f'{atom_ns}author')]
                abstract = entry.find(f'{atom_ns}summary').text.strip()
                pdf_link = next((l.get('href') for l in entry.findall(f'{atom_ns}link') if l.get('title') == 'pdf'),
                                None)
                if not pdf_link: continue
                arxiv_paper = {'id': entry.find(f'{atom_ns}id').text, 'title': title, 'abstract': abstract,
                               'authorships': [{'author': {'display_name': name}} for name in authors],
                               'best_oa_location': {'is_oa': True, 'pdf_url': pdf_link,
                                                    'source': {'display_name': 'arXiv'}},
                               'locations': [{'is_oa': True, 'pdf_url': pdf_link, 'source': {'display_name': 'arXiv'}}]}
                all_results.append(arxiv_paper)
                seen_titles.add(title.lower())
                new_papers_count += 1
            print(f"    [+] –ù–∞–π–¥–µ–Ω–æ {new_papers_count} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π.")
        except Exception as e:
            print(f"    [!] –û—à–∏–±–∫–∞: {e}")

    state['papers'] = all_results
    return state


# ========================= –£–ó–ï–õ –°–ö–ê–ß–ò–í–ê–ù–ò–Ø –ò –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò =========================
# ========================= –£–ó–ï–õ –°–ö–ê–ß–ò–í–ê–ù–ò–Ø –ò –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò =========================
def fetch_and_summarize_node(state: GraphState) -> GraphState:
    global token_count
    print("\n--- üì•‚úçÔ∏è –ê–ì–ï–ù–¢-–°–£–ú–ú–ê–†–ò–ó–ê–¢–û–†: –°–ö–ê–ß–ò–í–ê–Æ –ò –î–ï–õ–ê–Æ –†–ï–ó–Æ–ú–ï ---")
    papers = state['papers']
    existing_summaries = state['summaries']
    summarized_titles = {s['title'] for s in existing_summaries}
    new_papers = [p for p in papers if p.get('title') not in summarized_titles]

    if not new_papers:
        print("  [i] –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —ç—Ç–æ–º —Ü–∏–∫–ª–µ.")
        return state

    print(f"  [*] –ù–∞–π–¥–µ–Ω–æ {len(new_papers)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å—É–º–º—Ä–∏–∑–∞—Ü–∏–∏.")
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º StrOutputParser() –∏–∑ —Ü–µ–ø–æ—á–∫–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
    summarizer_chain = ChatPromptTemplate.from_template(SEARCH_SUMMARIZER_PROMPT) | llm
    new_summaries = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    for i, paper in enumerate(new_papers):
        if len(new_summaries) == MAX_ARTICLES_COUNT:
            break
        title = paper.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        authors = ", ".join([a.get("author", {}).get("display_name", "N/A") for a in paper.get("authorships", [])])
        print(f"\n  [{i + 1}/{len(new_papers)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: '{title[:70]}...'")
        pdf_url, source_display_name = None, "N/A"
        best_location = paper.get('best_oa_location')

        if best_location:
            source_dict = best_location.get('source')
            if source_dict:
                source_display_name = source_dict.get('display_name', 'N/A')

            pdf_url = best_location.get('pdf_url')
            if not pdf_url:
                landing_page_url = best_location.get('landing_page_url', '')
                if 'arxiv.org/abs' in landing_page_url: pdf_url = landing_page_url.replace('/abs/', '/pdf/')

        if not pdf_url:
            for loc in paper.get('locations', []):
                if loc and loc.get('pdf_url'):
                    pdf_url = loc['pdf_url']
                    source_display_name = loc.get('source', {}).get('display_name', 'N/A')
                    break
        text_content = None
        if pdf_url:
            print(f"    [*] –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å PDF –ø–æ URL: {pdf_url}")
            try:
                response = requests.get(pdf_url, headers=headers, timeout=45)
                response.raise_for_status()
                with fitz.open(stream=response.content, filetype="pdf") as doc:
                    pdf_text = "".join(page.get_text() for page in doc)
                if len(pdf_text.strip()) > 100:
                    text_content = pdf_text
                    print(f"    [+] PDF —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω.")
                else:
                    print(f"    [!] PDF —Å–∫–∞—á–∞–Ω, –Ω–æ –≤ –Ω–µ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞.")
            except Exception as e:
                print(f"    [!] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF {pdf_url}: {e}")
        is_arxiv_source = 'arxiv' in source_display_name.lower() or ('arxiv.org' in str(paper.get('id', '')))
        if not text_content and is_arxiv_source:
            arxiv_id_match = re.search(r'(\d{4}\.\d{4,5}(v\d+)?)', str(paper.get('id', '')))
            if arxiv_id_match:
                html_text = download_arxiv_html_article(arxiv_id_match.group(1))
                if html_text: text_content = html_text
            else:
                print(f"    [!] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å arXiv ID –¥–ª—è HTML-–∑–∞–≥—Ä—É–∑–∫–∏.")
        if not text_content and paper.get('abstract'):
            print("    [i] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é.")
            text_content = paper.get('abstract')
        if text_content:
            print("    [*] –°–æ–∑–¥–∞—é —Ä–µ–∑—é–º–µ...")
            try:
                # –¢–µ–ø–µ—Ä—å llm_response –±—É–¥–µ—Ç –æ–±—ä–µ–∫—Ç–æ–º AIMessage
                llm_response = summarizer_chain.invoke({"paper_text": text_content})
                if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
                    token_count += llm_response.usage_metadata.get('total_tokens', 0)
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ .content
                summary_text = llm_response.content
            except Exception as e:
                print(f"    [!] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑—é–º–µ: {e}")
                time.sleep(20)
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É
                llm_response = summarizer_chain.invoke({"paper_text": text_content})
                if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
                    token_count += llm_response.usage_metadata.get('total_tokens', 0)
                summary_text = llm_response.content

            new_summaries.append({"title": title, "authors": authors, "source": pdf_url or paper.get('id'),
                                  "summary": summary_text})
            print("    [+] –†–µ–∑—é–º–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ.")
        else:
            print(f"    [!] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏. –ü—Ä–æ–ø—É—Å–∫–∞—é.")

    state['summaries'] = existing_summaries + new_summaries
    return state


# ========================= –£–ó–ï–õ –í–ê–õ–ò–î–ê–¶–ò–ò –†–ï–ó–Æ–ú–ï =========================
def validate_summaries_node(state: GraphState) -> GraphState:
    global token_count
    print("\n--- ‚úÖ –ê–ì–ï–ù–¢-–í–ê–õ–ò–î–ê–¢–û–†: –ü–†–û–í–ï–†–Ø–Æ –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨ –ù–û–í–´–• –†–ï–ó–Æ–ú–ï ---")
    original_query = state['current_search_request'].input_query if state['current_search_request'] else ""

    all_summaries = state['summaries']
    previously_validated = state['validated_summaries']

    validated_titles = {s['title'] for s in previously_validated}
    summaries_to_validate = [s for s in all_summaries if s['title'] not in validated_titles]

    if not summaries_to_validate:
        print("  [i] –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ä–µ–∑—é–º–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ —ç—Ç–æ–º —Ü–∏–∫–ª–µ.")
        return state

    prompt = ChatPromptTemplate.from_template(VALIDATION_PROMPT)
    validation_chain = prompt | llm
    newly_validated_summaries = []
    print(
        f"  [*] –í–∞–ª–∏–¥–∏—Ä—É—é {len(summaries_to_validate)} –Ω–æ–≤—ã—Ö —Ä–µ–∑—é–º–µ...")

    for summary_data in summaries_to_validate:
        chain_input = {"original_query": original_query, "summary_text": summary_data['summary']}
        try:
            llm_response = validation_chain.invoke(chain_input)
            if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
                token_count += llm_response.usage_metadata.get('total_tokens', 0)
            result = llm_response.content.strip().lower()

        except Exception as e:
            print(f"  [!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ: {e}")
            time.sleep(20)
            llm_response = validation_chain.invoke(chain_input)
            if hasattr(llm_response, 'usage_metadata') and llm_response.usage_metadata:
                token_count += llm_response.usage_metadata.get('total_tokens', 0)
            result = llm_response.content.strip().lower()

        if "yes" in result:
            newly_validated_summaries.append(summary_data)

    state['validated_summaries'] = previously_validated + newly_validated_summaries

    print(
        f"\n  [+] –ò—Ç–æ–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ —Ü–∏–∫–ª–∞: {len(newly_validated_summaries)} –∏–∑ {len(summaries_to_validate)} —Ä–µ–∑—é–º–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É.")
    print(f"  [i] –í—Å–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—é–º–µ: {len(state['validated_summaries'])}")

    if state['current_search_request']:
        state['current_search_request'].results = state['validated_summaries']

    return state


# ========================= –£–ó–ï–õ –ü–†–ò–ù–Ø–¢–ò–Ø –†–ï–®–ï–ù–ò–ô =========================
def decide_to_continue_node(state: GraphState) -> str:
    print("\n--- ü§î –ê–ì–ï–ù–¢-–†–ï–®–ê–¢–ï–õ–¨: –ê–ù–ê–õ–ò–ó–ò–†–£–Æ –†–ï–ó–£–õ–¨–¢–ê–¢–´ ---")
    validated_count = len(state['validated_summaries'])
    cycle_count = state['search_cycles']
    print(f"  [i] –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {validated_count} (—Ü–µ–ª—å: {MIN_VALIDATED_ARTICLES})")
    print(f"  [i] –ü—Ä–æ—à–ª–æ —Ü–∏–∫–ª–æ–≤ –ø–æ–∏—Å–∫–∞: {cycle_count} (–ª–∏–º–∏—Ç: {MAX_SEARCH_CYCLES})")

    if validated_count >= MIN_VALIDATED_ARTICLES:
        print("  [+] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–∞—Ç–µ–π –Ω–∞–π–¥–µ–Ω–æ. –ü–µ—Ä–µ—Ö–æ–∂—É –∫ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Ç—á–µ—Ç–∞.")
        return "prepare_report"
    if cycle_count >= MAX_SEARCH_CYCLES:
        print("  [!] –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ü–∏–∫–ª–æ–≤ –ø–æ–∏—Å–∫–∞. –ü–µ—Ä–µ—Ö–æ–∂—É –∫ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Ç—á–µ—Ç–∞ —Å —Ç–µ–º, —á—Ç–æ –µ—Å—Ç—å.")
        return "prepare_report"
    else:
        print("  [!] –ù–∞–π–¥–µ–Ω–æ –º–∞–ª–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –ó–∞–ø—É—Å–∫–∞—é –Ω–æ–≤—ã–π —Ü–∏–∫–ª –ø–æ–∏—Å–∫–∞.")
        return "continue_search"


# ========================= –£–ó–ï–õ –ü–û–î–ì–û–¢–û–í–ö–ò –§–ò–ù–ê–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê =========================
def prepare_final_report_node(state: GraphState) -> GraphState:
    print("\n--- üìã –ì–û–¢–û–í–õ–Æ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ ---")
    validated_summaries = state['validated_summaries']

    if state['current_search_request'] and state['current_search_request'].results:
        state['search_history'].append(state['current_search_request'])

    if not validated_summaries:
        report = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ü–∏–∫–ª–æ–≤ –ø–æ–∏—Å–∫–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏, —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
        print("  [!] –ù–µ—Ç –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞.")
        state['final_report'] = report
        return state

    query = state['current_search_request'].input_query if state['current_search_request'] else "–∑–∞–ø—Ä–æ—Å"
    report_parts = [
        f"**–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É: '{query}'**\n\n–ù–∞–π–¥–µ–Ω–æ {len(validated_summaries)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:\n"]

    for summary_data in validated_summaries:
        report_part = (f"### üìñ –°—Ç–∞—Ç—å—è: {summary_data['title']}\n\n"
                       f"**–ê–≤—Ç–æ—Ä—ã:** {summary_data['authors']}\n\n"
                       f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {summary_data['source']}\n\n"
                       f"**–†–µ–∑—é–º–µ:**\n{summary_data['summary']}\n")
        report_parts.append(report_part)

    final_report = "\n---\n".join(report_parts)
    print(f"  [+] –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∏–∑ {len(validated_summaries)} —Ä–µ–∑—é–º–µ.")
    state['final_report'] = final_report
    return state


# --- 5. –°–ë–û–†–ö–ê –ò –ó–ê–ü–£–°–ö –ì–†–ê–§–ê ---
def compile_workflow():
    workflow = StateGraph(GraphState)
    workflow.add_node("plan_search_queries", plan_search_queries_node)
    workflow.add_node("search_openalex", search_openalex_node)
    workflow.add_node("search_arxiv", search_arxiv_node)
    workflow.add_node("fetch_and_summarize", fetch_and_summarize_node)
    workflow.add_node("validate_summaries", validate_summaries_node)
    workflow.add_node("decide_to_continue", decide_to_continue_node)
    workflow.add_node("prepare_final_report", prepare_final_report_node)

    workflow.set_entry_point("plan_search_queries")
    workflow.add_edge("plan_search_queries", "search_openalex")
    workflow.add_edge("search_openalex", "search_arxiv")
    workflow.add_edge("search_arxiv", "fetch_and_summarize")
    workflow.add_edge("fetch_and_summarize", "validate_summaries")
    workflow.add_conditional_edges(
        "validate_summaries",
        decide_to_continue_node,
        {"continue_search": "plan_search_queries", "prepare_report": "prepare_final_report"}
    )
    workflow.add_edge("prepare_final_report", END)
    app = workflow.compile()
    return app


def node_make_research(state: GraphState) -> Dict:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π —É–∑–µ–ª-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è.
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–¥-–≥—Ä–∞—Ñ –ø–æ–∏—Å–∫–∞ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.
    """
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥-–≥—Ä–∞—Ñ –ø–æ–∏—Å–∫–∞
    final_report, request = make_research(state['current_search_request'].input_query, state)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞
    return {
        'current_search_request': None,  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        'papers': [],
        'summaries': [],
        'validated_summaries': [],
        'final_report': final_report,  # –≠—Ç–æ –ø–æ–ª–µ —Å–µ–π—á–∞—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∞–ª—å—à–µ, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        # 'search_history' —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω –≤–Ω—É—Ç—Ä–∏ make_research, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
    }


def make_research(query, state: GraphState) -> tuple[str, SearchRequest]:
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø–æ–¥-–≥—Ä–∞—Ñ–∞
    initial_search_state = state.copy()
    initial_search_state['current_search_request'] = SearchRequest(input_query=query)
    app = compile_workflow()
    state['papers'] = []
    state['summaries'] = []
    state['validated_summaries'] = []

    final_state_data = None
    recursion_limit = (MAX_SEARCH_CYCLES * 5) + 5

    for event in app.stream(initial_search_state, config={"recursion_limit": recursion_limit}):
        for node_name, state_update in event.items():
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–∑ –ø–æ–¥-–≥—Ä–∞—Ñ–∞
            for key, value in state_update.items():
                if key in state:
                    state[key] = value
            final_state_data = state_update

    print("\n\n" + "=" * 80 + "\n‚úÖ –†–ê–ë–û–¢–ê –ü–û–ò–°–ö–û–í–û–ì–û –ê–ì–ï–ù–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê ‚úÖ\n" + "=" * 80 + "\n")

    if final_state_data:
        final_report = final_state_data.get('final_report', "–û—Ç—á–µ—Ç –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
        # `search_history` –≤ `state` —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω
        last_search_request = next((s for s in reversed(state.get('search_history', [])) if s.input_query == query),
                                   None)
        return final_report, last_search_request
    else:
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞.", SearchRequest(input_query=query)
